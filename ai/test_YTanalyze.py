import nltk
nltk.download('punkt')

from youtube_transcript_api import YouTubeTranscriptApi
import nltk, openai, time, torch
from nltk.tokenize import sent_tokenize
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import requests

from dotenv import load_dotenv
import os
load_dotenv()

# Setup
nltk.download("punkt")
openai.api_key = os.getenv("OPENAI_API_KEY")
ASSISTANT_ID = os.getenv("ASSISTANT_ID")
NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")

TRUSTED_SOURCES = ["chosun", "hani", "khan", "donga", "joongang", "mbn", "sbs", "ytn", "kbs", "mbc", "jtbc", "ohmynews", "newsis", "yna", "hankookilbo"]

# ëª¨ë¸ ë¡œë“œ
MODEL_PATH = "./model/kcbert_final_model"
model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

#  1. ìœ íŠœë¸Œ ìë§‰ ì¶”ì¶œ
def get_transcript(video_url):
    video_id = video_url.split("v=")[-1]
    try:
        transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=["ko", "en"])
        return " ".join([t["text"] for t in transcript])
    except Exception as e:
        print(f"[ ìë§‰ ì˜¤ë¥˜] {e}")
        return None

#  2. GPT Assistant ìš”ì•½ ìš”ì²­
def summarize_with_assistant(text):
    # 1. Thread ìƒì„±
    thread = openai.beta.threads.create()
    thread_id = thread.id

    # 2. ë©”ì‹œì§€ ì „ì†¡ ë° Assistant ì‹¤í–‰
    openai.beta.threads.messages.create(
        thread_id=thread_id,
        role="user",
        content=text
    )
    run = openai.beta.threads.runs.create(
        thread_id=thread_id,
        assistant_id=ASSISTANT_ID
    )

    # 3. ì‹¤í–‰ ìƒíƒœ ëŒ€ê¸°
    while run.status in ["queued", "in_progress"]:
        run = openai.beta.threads.runs.retrieve(
            thread_id=thread_id,
            run_id=run.id,
        )
        time.sleep(1)

    # 4. ê²°ê³¼ ë©”ì‹œì§€ ë°›ì•„ì˜¤ê¸°
    messages = openai.beta.threads.messages.list(thread_id=thread_id, order="asc")
    result = messages.data[-1].content[0].text.value
    return result

#  3. ì •ì¹˜ì„±í–¥ ì˜ˆì¸¡
def predict_bias(sentences):
    inputs = tokenizer(sentences, return_tensors="pt", padding=True, truncation=True, max_length=128).to(device)
    with torch.no_grad():
        outputs = model(**inputs)
        preds = torch.argmax(outputs.logits, dim=1).cpu().tolist()
    reverse_map = {0: -2, 1: -1, 2: 0, 3: 1, 4: 2}
    return [reverse_map[p] for p in preds]

# 4.  ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰
def search_news_articles(keyword, max_articles=3):
    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET,
    }
    params = {
        "query": keyword,
        "display": 10,
        "sort": "date"
    }
    response = requests.get(url, headers=headers, params=params)
    results = []
    if response.status_code == 200:
        items = response.json().get("items", [])
        for item in items:
            # ğŸ”» ì—¬ê¸° ìˆ˜ì •ëœ ë¶€ë¶„: í•„í„°ë§ ì—†ì´ ëª¨ë‘ í¬í•¨
            results.append({
                "title": item.get("title"),
                "link": item.get("link")
            })
            if len(results) >= max_articles:
                break
    return results



def extract_keywords_from_summary(summary_text):
    if "[KEYWORDS]" in summary_text:
        parts = summary_text.split("[KEYWORDS]")
        keyword_block = parts[-1].strip()
        queries= [kw.strip() for kw in keyword_block.split(",") if kw.strip()]
        return queries
    return []




#   ì „ì²´ íë¦„ í†µí•©
def analyze_youtube_political_bias(url):
    print(f"\n ìœ íŠœë¸Œ ì²˜ë¦¬ ì¤‘: {url}")
    transcript = get_transcript(url)
    if not transcript:
        print("ìë§‰ì´ ì—†ê±°ë‚˜ ì¶”ì¶œ ì‹¤íŒ¨.")
        return

    print(" GPT Assistantì—ê²Œ ìš”ì•½ ìš”ì²­ ì¤‘...")
    summary = summarize_with_assistant(transcript)
    print("\n ìš”ì•½ ê²°ê³¼:\n", summary)

    content_only = summary.split("[KEYWORDS]")[0].strip()
    sentences = sent_tokenize(content_only)
    print(f"\nì´ {len(sentences)}ê°œì˜ ë¬¸ì¥ì„ ë¶„ì„í•©ë‹ˆë‹¤...")


    bias_scores = predict_bias(sentences)
    for s, b in zip(sentences, bias_scores):
        print(f"[{b:+}] {s}")

    avg = round(sum(bias_scores) / len(bias_scores), 3)
    print(f"\n í‰ê·  ì •ì¹˜ í¸í–¥ë„: {avg:+}")

    queries = extract_keywords_from_summary(summary)
    related_articles = {kw: search_news_articles(kw) for kw in queries}

    print("\n í‚¤ì›Œë“œë³„ ê´€ë ¨ ë‰´ìŠ¤ ê¸°ì‚¬:")
    for kw, articles in related_articles.items():
        print(f"\n{kw}:")
        for art in articles:
            print(f"- {art['title']}\n  {art['link']}")



# ì‚¬ìš©
if __name__ == "__main__":
    youtube_url = "https://www.youtube.com/watch?v=H52SDhRpH5A"
    analyze_youtube_political_bias(youtube_url)
