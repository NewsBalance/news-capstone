from flask import Flask, request, jsonify
import nltk
nltk.download('punkt')

import subprocess
import json
import re
from youtube_transcript_api import YouTubeTranscriptApi
import nltk, openai, time, torch
from nltk.tokenize import sent_tokenize
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import requests

from dotenv import load_dotenv
import os

load_dotenv()

# Setup
nltk.download("punkt")
openai.api_key = os.getenv("OPENAI_API_KEY")
ASSISTANT_ID = os.getenv("ASSISTANT_ID")
ASSISTANT2_ID = os.getenv("ASSISTANT2_ID")
NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")

TRUSTED_SOURCES = ["chosun", "hani", "khan", "donga", "joongang", "mbn", "sbs", "ytn", "kbs", "mbc", "jtbc", "ohmynews", "newsis", "yna", "hankookilbo"]



# ëª¨ë¸ ë¡œë“œ
MODEL_PATH = "./model/kcbert_second_model"
model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

app = Flask(__name__)


#  1. ìœ íŠœë¸Œ ìë§‰ ì¶”ì¶œ

def get_transcript(video_url, lang="ko"):
    # video_id ì¶”ì¶œ
    match = re.search(r"[?&]v=([a-zA-Z0-9_-]{11})", video_url)
    if not match:
        return None, "âŒ ìœ íš¨í•˜ì§€ ì•Šì€ ìœ íŠœë¸Œ URL í˜•ì‹ì…ë‹ˆë‹¤."
    video_id = match.group(1)

    try:
        # yt-dlpë¡œ ìë™ ìƒì„± ìë§‰ URL ì¶”ì¶œ
        result = subprocess.run(
            ["yt-dlp", "-J", video_url],
            capture_output=True, text=True, check=True
        )
        video_info = json.loads(result.stdout)
        subtitles = video_info.get("automatic_captions", {})
        if lang not in subtitles:
            return None, f"âŒ '{lang}' ì–¸ì–´ì˜ ìë™ ìƒì„± ìë§‰ì´ ì—†ìŠµë‹ˆë‹¤."

        subtitle_url = subtitles[lang][0]["url"]

        # json3 ìë§‰ ê·¸ëŒ€ë¡œ ìš”ì²­
        response = requests.get(subtitle_url)
        if response.status_code != 200:
            return None, "âŒ ìë§‰ ë°ì´í„° ìš”ì²­ ì‹¤íŒ¨"

        data = response.json()
        if "events" not in data:
            return None, "âŒ ìë§‰ ì´ë²¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

        texts = []
        for event in data["events"]:
            if "segs" in event:
                seg_text = "".join([seg.get("utf8", "") for seg in event["segs"]])
                texts.append(seg_text.strip())

        full_text = " ".join(texts)
        return full_text, None

    except subprocess.CalledProcessError as e:
        return None, f"âŒ yt-dlp ì‹¤í–‰ ì‹¤íŒ¨: {e}"
    except Exception as e:
        return None, f"âŒ ìë§‰ ì²˜ë¦¬ ì‹¤íŒ¨: {e}"

#  2. GPT Assistant ìš”ì•½ ìš”ì²­
def summarize_with_assistant(text):
    # 1. Thread ìƒì„±
    thread = openai.beta.threads.create()
    thread_id = thread.id

    # 2. ë©”ì‹œì§€ ì „ì†¡ ë° Assistant ì‹¤í–‰
    openai.beta.threads.messages.create(
        thread_id=thread_id,
        role="user",
        content=text
    )
    run = openai.beta.threads.runs.create(
        thread_id=thread_id,
        assistant_id=ASSISTANT_ID
    )

    # 3. ì‹¤í–‰ ìƒíƒœ ëŒ€ê¸°
    while run.status in ["queued", "in_progress"]:
        run = openai.beta.threads.runs.retrieve(
            thread_id=thread_id,
            run_id=run.id,
        )
        time.sleep(1)

    # 4. ê²°ê³¼ ë©”ì‹œì§€ ë°›ì•„ì˜¤ê¸°
    messages = openai.beta.threads.messages.list(thread_id=thread_id, order="asc")
    result = messages.data[-1].content[0].text.value
    return result

#  3. ì •ì¹˜ì„±í–¥ ì˜ˆì¸¡
def predict_bias(sentences):
    inputs = tokenizer(sentences, return_tensors="pt", padding=True, truncation=True, max_length=128).to(device)
    with torch.no_grad():
        outputs = model(**inputs)
        preds = torch.argmax(outputs.logits, dim=1).cpu().tolist()
    reverse_map = {0: -2, 1: -1, 2: 0, 3: 1, 4: 2}
    return [reverse_map[p] for p in preds]

# 4.  ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰
def search_news_articles(keyword, max_articles=3):
    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET,
    }
    params = {
        "query": keyword,
        "display": 10,
        "sort": "sim"
    }
    response = requests.get(url, headers=headers, params=params)
    results = []
    if response.status_code == 200:
        items = response.json().get("items", [])
        for item in items:
            # ğŸ”» ì—¬ê¸° ìˆ˜ì •ëœ ë¶€ë¶„: í•„í„°ë§ ì—†ì´ ëª¨ë‘ í¬í•¨
            results.append({
                "title": item.get("title"),
                "link": item.get("link")
            })
            if len(results) >= max_articles:
                break
    return results

def extract_keywords_from_summary(summary_text):
    if "[KEYWORDS]" in summary_text:
        parts = summary_text.split("[KEYWORDS]")
        keyword_block = parts[-1].strip()
        queries= [kw.strip() for kw in keyword_block.split(",") if kw.strip()]
        return queries
    elif "[Query]" in summary_text:
        parts = summary_text.split("[Query]")
        keyword_block = parts[-1].strip()
        queries= [kw.strip() for kw in keyword_block.split(",") if kw.strip()]
        return queries
    return []


#  ìœ íŠœë¸Œ ì»¨í…ì¸  ìš”ì•½ ì—”ë“œí¬ì¸íŠ¸
@app.route('/summarize', methods=['POST'])
def summarize_endpoint():
    data = request.get_json()
    url = data.get("url")
    transcript, err = get_transcript(url)
    
    if err or not transcript or not isinstance(transcript, str) or transcript.strip() == "":
        return jsonify({"error": err or "ìë§‰ ì¶”ì¶œ ì‹¤íŒ¨"}), 400

    try:
        # yt-dlp -J í˜¸ì¶œë¡œ ë©”íƒ€ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        info_proc = subprocess.run(
            ["yt-dlp", "-J", url],
            capture_output=True, text=True, check=True
        )
        info_json = json.loads(info_proc.stdout)
        video_title = info_json.get("title", "")
    except Exception as e:
        app.logger.warning(f"ì œëª© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        video_title = ""
        
    try:
        summary = summarize_with_assistant(transcript)
    except Exception as e:
        import traceback
        traceback.print_exc()
        return jsonify({"error": f"GPT Assistant ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}"}), 500
    content_only = summary.split("[KEYWORDS]")[0].strip()
    sentences = sent_tokenize(content_only)
    scores = predict_bias(sentences)
    avg = sum(scores) / len(scores) if scores else 0

    queries = extract_keywords_from_summary(summary)
    related_articles = {kw: search_news_articles(kw) for kw in queries}

    related_articles_result = []
    for articles in related_articles.values():
        for art in articles:
            related_articles_result.append({
                "title": art["title"],
                "link": art["link"]
            })

    result = {
        "url": url,
        "title": video_title,
        "biasScore": avg,
        "summarySentences": [
            {"content": s, "score": b}
            for s, b in zip(sentences, scores)
        ],
        "relatedArticles": related_articles_result,
        "keywords": list({kw_part for kw in queries for kw_part in kw.split("+")})
    }
    return jsonify(result)


#  1. GPT Assistant í† ë¡ ì ë°œì–¸ ìš”ì•½ ìš”ì²­
def dabate_summarize_with_assistant(text):
    # 1. Thread ìƒì„±
    thread = openai.beta.threads.create()
    thread_id = thread.id

    # 2. ë©”ì‹œì§€ ì „ì†¡ ë° Assistant ì‹¤í–‰
    openai.beta.threads.messages.create(
        thread_id=thread_id,
        role="user",
        content=text
    )
    run = openai.beta.threads.runs.create(
        thread_id=thread_id,
        assistant_id=ASSISTANT2_ID
    )

    # 3. ì‹¤í–‰ ìƒíƒœ ëŒ€ê¸°
    while run.status in ["queued", "in_progress"]:
        run = openai.beta.threads.runs.retrieve(
            thread_id=thread_id,
            run_id=run.id,
        )
        time.sleep(1)

    # 4. ê²°ê³¼ ë©”ì‹œì§€ ë°›ì•„ì˜¤ê¸°
    messages = openai.beta.threads.messages.list(thread_id=thread_id, order="asc")
    result = messages.data[-1].content[0].text.value
    return result


#  í† ë¡ ì ë°œì–¸ ìš”ì§€ ìš”ì•½ ì—”ë“œí¬ì¸íŠ¸
@app.route('/debate/summarize', methods=['POST'])
def dabate_summarize_endpoint():
    data = request.get_json()
    print(">>> ìˆ˜ì‹  ë°ì´í„°:", data)
    
    # requestì—ì„œ í•„ìš”í•œ ë°ì´í„° ì¶”ì¶œ
    message = data.get("messages", [])
    combined_message = "\n".join([m.get("text", "") for m in message])
    
    summarize_message = dabate_summarize_with_assistant(combined_message)

    if "[Query]" in summarize_message:
        summarize_message_clean = summarize_message.split("[Query]")[0].strip()
        
    queries = extract_keywords_from_summary(summarize_message)
    related_articles = {kw: search_news_articles(kw) for kw in queries}

    
    related_articles_result = []
    for articles in related_articles.values():  
        for art in articles:
            related_articles_result.append({
                "title": art["title"],
                "link": art["link"]
            })


    # ë°ì´í„° ë°˜í™˜
    result = {
        "summarizemessage": summarize_message_clean,
        "relatedArticles": related_articles_result,
        "keywords": list({kw_part for kw in queries for kw_part in kw.split("+")})
        
    }
    return jsonify(result)


if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
    
